{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "278696 74603\nusers with < 5 interactoins are removed\nitems with < 10 interactoins are removed\nnum of users:5801, num of items:797\nsuccressfully created sequencial data! head: user_id\nA0028518312V9G9GA1SDL                             [B0001FTVD6, B0002MDTVS]\nA00338543M2OZPUWO9ZRU     [B0002E1G5C, B0002D0CEO, B0002E1H9W, B0002IHFVM]\nA00625243BI8W1SSZNLMD    [B000SAC5PA, B0009G1E0K, B000PO30QM, B0018TIAD...\nA01458943UKX2HCM0VVN0     [B003NJF1G8, B001LU1SFO, B003VWJ2K8, B008GS3XLQ]\nA04561483EYOO5BSJ3VEU                 [B005FKF1PY, B007T8CUNG, B009EOKTCM]\nName: item_id, dtype: object\nuser_id\nA13IKQCJKFAP5S    [B000BUDO48, B002433RXI, B0009G1E0K, B0002GLDQ...\nA15TYOEWBQYF0X    [B0002E2XCW, B0002GLDQM, B00BTGMI5O, B005PNXT6...\nA15WZCSME5X74S    [B0002E1G5C, B003VWJ2K8, B003B01QL8, B002AQNGI...\nA164BJ2NU1NSJZ    [B000PO30QM, B001L8NGJ2, B003AYRBWI, B0002E2GM...\nA16Z3HTUIYPDH8    [B000B6DHAS, B0002GYW4C, B000RNB720, B0002CZSJ...\nA1BH17V8BKLES     [B0002M6CVC, B000N5MK8M, B0002CZSJO, B0002D0CE...\nA1C0O09LOLVI39    [B001BY4RQM, B0002E2NEK, B0002II6V0, B002RLLD8...\nA1CL807EOUPVP1    [B0002F7K7Y, B000068NSX, B0009DXEEM, B0016MJ1T...\nA1DVUFG2QSJ6IK    [B002Q0WSO8, B0002E2GMY, B003AYK8XW, B002IC1D5...\nA1DZDG9AT98298    [B0009G1E0K, B0073XCXHA, B0044R1M0M, B005PNXT6...\nName: item_id, dtype: object\n107\n0\n100\n0\n1\n107 107\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "UP = 0\n",
    "LEFT = 1\n",
    "OK = 2\n",
    "batch_size = 100\n",
    "K = 0\n",
    "def read_user_rating_records():\n",
    "    col_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    data_records = pd.read_csv(dir_path + rating_file, sep=',', names=col_names, engine='python')\n",
    "    return data_records\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "def remove_infrequent_items(data, min_counts=20):\n",
    "    df = deepcopy(data)\n",
    "    counts = df['item_id'].value_counts()\n",
    "    df = df[df[\"item_id\"].isin(counts[counts >= min_counts].index)]\n",
    "\n",
    "    print(\"items with < {} interactoins are removed\".format(min_counts))\n",
    "    # print(df.describe())\n",
    "    return df\n",
    "\n",
    "def remove_infrequent_users(data, min_counts=5):\n",
    "    df = deepcopy(data)\n",
    "    counts = df['user_id'].value_counts()\n",
    "    df = df[df[\"user_id\"].isin(counts[counts >= min_counts].index)]\n",
    "\n",
    "    print(\"users with < {} interactoins are removed\".format(min_counts))\n",
    "    # print(df.describe())\n",
    "    return df\n",
    "def convert_data(data):\n",
    "    # for each user, sort by timestamps\n",
    "    df = deepcopy(data)\n",
    "    df_ordered = df.sort_values(['timestamp'], ascending=True)\n",
    "    data = df_ordered.groupby('user_id')['item_id'].apply(list)\n",
    "    #print(data)\n",
    "    #time_l = df_ordered.groupby('user')['checkin_time'].apply(list)\n",
    "    #print(time_l)\n",
    "    print(\"succressfully created sequencial data! head:\", data.head(5))\n",
    "    unique_data = df_ordered.groupby('user_id')['item_id'].nunique()\n",
    "    data = data[unique_data[unique_data >= 10].index]\n",
    "    print(data[:10])\n",
    "    print(len(data))\n",
    "    return data\n",
    "def seqence_similarity(seq1,seq2):\n",
    "    len1 = len(seq1)\n",
    "    len2 = len(seq2)\n",
    "    dp = np.zeros((len1+1,len2+1))\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0]=i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j]=j\n",
    "    for i in range(1,len1+1):\n",
    "        for j in range(1,len2+1):\n",
    "            delta=0 if seq1[i-1]==seq2[j-1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j - 1] + delta, min(dp[i-1][j] + 1, dp[i][j - 1] + 1))\n",
    "    length = max(len1,len2)\n",
    "    distance = dp[len1][len2]\n",
    "    similarity = (length-distance)/distance\n",
    "    return similarity\n",
    "def LCS(seq1,seq2):\n",
    "    if (set(seq1).intersection(set(seq2)))==set():\n",
    "        return []\n",
    "    m = [[0 for x in range(len(seq2)+1)] for y in range(len(seq1)+1)]\n",
    "    d = [[None for x in range(len(seq2)+1)] for y in range(len(seq1)+1)]\n",
    "    for p1 in range(len(seq1)):\n",
    "        for p2 in range(len(seq2)):\n",
    "            if seq1[p1] == seq2[p2]:\n",
    "                m[p1+1][p2+1] = m[p1][p2]+1\n",
    "                d[p1+1][p2+1] = OK\n",
    "            elif m[p1+1][p2]>m[p1][p2+1]:\n",
    "                m[p1+1][p2+1]=m[p1+1][p2]\n",
    "                d[p1+1][p2+1] = LEFT\n",
    "            else:\n",
    "                m[p1+1][p2+1]=m[p1][p2+1]\n",
    "                d[p1+1][p2+1] = UP\n",
    "    (p1,p2) = (len(seq1),len(seq2))\n",
    "    index = []\n",
    "    while m[p1][p2]:\n",
    "        direction = d[p1][p2]\n",
    "        #print(p1, p2)\n",
    "        #print(direction)\n",
    "        if direction == OK:\n",
    "            index.append((p1-1,p2-1))\n",
    "            p1 -= 1\n",
    "            p2 -= 1\n",
    "        elif direction == LEFT:\n",
    "            p2 -= 1\n",
    "        elif direction == UP:\n",
    "            p1 -= 1\n",
    "    index.reverse()\n",
    "    return index\n",
    "\n",
    "\n",
    "def merging(seq1,seq2,index):\n",
    "    index_first = [x[0] for x in index]\n",
    "    index_second = [x[1] for x in index]\n",
    "    new_seq = []\n",
    "    parts = len(index)\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    for i in range(parts):\n",
    "        idx1 = index_first[i]\n",
    "        idx2 = index_second[i]\n",
    "        new_seq+=seq1[p1:idx1]+seq2[p2:idx2]+[seq1[idx1]]\n",
    "        p1 = idx1+1\n",
    "        p2 = idx2+1\n",
    "\n",
    "    new_seq+=seq1[p1:len(seq1)]+seq2[p2:len(seq2)]\n",
    "    return new_seq\n",
    "\n",
    "\n",
    "dir_path = './'\n",
    "rating_file = 'ratings_Musical_Instruments.csv'\n",
    "\n",
    "data_records = read_user_rating_records()\n",
    "data_records.loc[data_records.rating < 4, 'rating'] = 0\n",
    "data_records.loc[data_records.rating >= 4, 'rating'] = 1\n",
    "data_records = data_records[data_records.rating > 0]\n",
    "print(len(data_records['user_id'].unique()), len(data_records['item_id'].unique()))\n",
    "\n",
    "filtered_data = remove_infrequent_users(data_records, 5)\n",
    "filtered_data = remove_infrequent_items(filtered_data, 10)\n",
    "print('num of users:{}, num of items:{}'.format(len(filtered_data['user_id'].unique()), len(filtered_data['item_id'].unique())))\n",
    "\n",
    "\n",
    "seq_data = convert_data(filtered_data)\n",
    "batch_num = len(seq_data)//batch_size\n",
    "index = 0\n",
    "series = []\n",
    "for i in range(0,batch_num+1):\n",
    "    print(index)\n",
    "    end = index + 100 if index + 100 <= len(seq_data) else len(seq_data)\n",
    "    series.append(seq_data[index:end])\n",
    "    index += 100\n",
    "merged_sequence = pd.Series()\n",
    "for b in range(batch_num+1):\n",
    "    for k in range(K):\n",
    "        MAX = 0\n",
    "        MAX_A,MAX_B = -1,-1\n",
    "        for i  in range(len(series[b])):\n",
    "            for j in range(i):\n",
    "                #print(i,j)\n",
    "                if i != j:\n",
    "                    similarity = seqence_similarity(series[b][i],series[b][j])\n",
    "                    if similarity>MAX:\n",
    "                        MAX = similarity\n",
    "                        MAX_A = i\n",
    "                        MAX_B = j\n",
    "                        print('update similarity, MAX S={} comes from {} and {}'.format(MAX,MAX_A,MAX_B))\n",
    "        seq1 = series[b][MAX_A]\n",
    "        seq2 = series[b][MAX_B]\n",
    "        user_A = series[b].index[MAX_A]\n",
    "        user_B = series[b].index[MAX_B]\n",
    "        series[b] = series[b].drop([user_A,user_B])\n",
    "        if len(seq1)<len(seq2):\n",
    "            index = LCS(seq2,seq1)\n",
    "            seq = merging(seq2,seq1,index)\n",
    "            series[b][user_B]=seq\n",
    "        else:\n",
    "            index = LCS(seq1,seq2)\n",
    "            seq = merging(seq1,seq2,index)\n",
    "            series[b][user_A] = seq\n",
    "        print('In batch {}, merging step {}, user {} and {} are merged'.format(b,k+1,user_A,user_B))\n",
    "for b in range(batch_num+1):\n",
    "    print(b)\n",
    "    merged_sequence = pd.concat([merged_sequence,series[b]])\n",
    "print(len(merged_sequence),len(seq_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10\n10\n"
    }
   ],
   "source": [
    "print(len(merged_sequence[105]))\n",
    "print(len(seq_data[105]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "107 589\n"
    }
   ],
   "source": [
    "user_item_dict = merged_sequence.to_dict()\n",
    "user_mapping = []\n",
    "item_set = set()\n",
    "for user_id, item_list in merged_sequence.iteritems():\n",
    "    user_mapping.append(user_id)\n",
    "    for item_id in item_list:\n",
    "        item_set.add(item_id)\n",
    "item_mapping = list(item_set)\n",
    "\n",
    "print(len(user_mapping), len(item_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "107 589\n"
    }
   ],
   "source": [
    "user_item_dict = seq_data.to_dict()\n",
    "user_mapping = []\n",
    "item_set = set()\n",
    "for user_id, item_list in seq_data.iteritems():\n",
    "    user_mapping.append(user_id)\n",
    "    for item_id in item_list:\n",
    "        item_set.add(item_id)\n",
    "item_mapping = list(item_set)\n",
    "\n",
    "print(len(user_mapping), len(item_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[345, 51, 90, 117, 257, 290, 396, 19, 288, 185, 463, 25, 379, 587], [199, 117, 545, 384, 261, 428, 259, 459, 194, 313, 211, 185, 252, 499, 420, 579, 271, 90, 270, 393, 183, 326, 408, 260, 357], [428, 463, 224, 200, 113, 219, 448, 117, 286, 242], [290, 303, 529, 183, 305, 27, 360, 250, 35, 178], [324, 517, 585, 498, 387, 298, 251, 375, 75, 579]]\n"
    }
   ],
   "source": [
    "def generate_inverse_mapping(data_list):\n",
    "    inverse_mapping = dict()\n",
    "    for inner_id, true_id in enumerate(data_list):\n",
    "        inverse_mapping[true_id] = inner_id\n",
    "    return inverse_mapping\n",
    "\n",
    "def convert_to_inner_index(user_records, user_mapping, item_mapping):\n",
    "    inner_user_records = []\n",
    "    user_inverse_mapping = generate_inverse_mapping(user_mapping)\n",
    "    item_inverse_mapping = generate_inverse_mapping(item_mapping)\n",
    "\n",
    "    for user_id in range(len(user_mapping)):\n",
    "        real_user_id = user_mapping[user_id]\n",
    "        item_list = list(user_records[real_user_id])\n",
    "        for index, real_item_id in enumerate(item_list):\n",
    "            item_list[index] = item_inverse_mapping[real_item_id]\n",
    "        inner_user_records.append(item_list)\n",
    "\n",
    "    return inner_user_records, user_inverse_mapping, item_inverse_mapping\n",
    "\n",
    "inner_data_records, user_inverse_mapping, item_inverse_mapping = convert_to_inner_index(user_item_dict, user_mapping, item_mapping)\n",
    "print(inner_data_records[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(inner_data_records, '../data/dataset/Amazon/MI/MI_K={}_BS={}_item_sequences'.format(K,batch_size))\n",
    "save_obj(user_mapping, '../data/dataset/Amazon/MI/MI_K={}_BS={}_user_mapping'.format(K,batch_size))\n",
    "save_obj(item_mapping, '../data/dataset/Amazon/MI/MI_K={}_BS={}_item_mapping'.format(K,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def generate_rating_matrix(train_set, num_users, num_items):\n",
    "    # three lists are used to construct sparse matrix\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for user_id, article_list in enumerate(train_set):\n",
    "        for article in article_list:\n",
    "            row.append(user_id)\n",
    "            col.append(article)\n",
    "            data.append(1)\n",
    "\n",
    "    row = np.array(row)\n",
    "    col = np.array(col)\n",
    "    data = np.array(data)\n",
    "    rating_matrix = csr_matrix((data, (row, col)), shape=(num_users, num_items))\n",
    "\n",
    "    return rating_matrix\n",
    "\n",
    "rating_matrix = generate_rating_matrix(inner_data_records, len(user_mapping), len(item_mapping))\n",
    "rating_matrix = rating_matrix.transpose()\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "relation_matrix = cosine_similarity(rating_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.08317669172932331"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "rating_matrix.nnz / float(len(user_mapping) * len(item_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "70\n152\n1.0\n[0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.5        0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.4472136  0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.37796447 0.70710678 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.37796447 0.37796447 0.         0.         0.\n 0.         0.         0.         0.33333333 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         1.         0.         0.         0.\n 0.         0.         1.         0.         0.57735027 0.\n 1.         0.5        0.         0.         0.         0.\n 0.         0.        ]\n[0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.70710678 0.40824829 0.         0.\n 0.         0.70710678 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.40824829 0.         0.\n 0.         0.         0.         0.         0.         0.70710678\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.31622777 0.\n 0.         0.70710678 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.40824829\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.31622777 0.         0.\n 0.         0.         0.         0.         0.         0.40824829\n 0.         0.         0.31622777 0.         0.         0.\n 0.2236068  0.         0.         0.         0.         0.40824829\n 0.         0.         0.         1.         0.         0.\n 0.         0.         0.         0.         0.28867513 0.\n 0.28867513 0.         0.         0.         0.         0.35355339\n 0.         0.         0.         0.         0.         0.\n 0.         0.28867513 0.         0.         0.         0.\n 0.         0.         0.         0.28867513 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.5        0.        ]\n"
    }
   ],
   "source": [
    "np.fill_diagonal(relation_matrix, 0)\n",
    "max_count = 0\n",
    "for i in range(len(item_mapping)):\n",
    "    max_count = max(np.count_nonzero((relation_matrix[i] >= 0.2) == True), max_count)\n",
    "    \n",
    "print (max_count)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(item_mapping)):\n",
    "    if np.count_nonzero((relation_matrix[i] >= 0.2) == True) > 0:\n",
    "        count += 1\n",
    "\n",
    "print (count)\n",
    "print (np.max(relation_matrix))\n",
    "print (relation_matrix[0])\n",
    "print (relation_matrix[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "28 152\n"
    }
   ],
   "source": [
    "relation_matrix[relation_matrix < 0.2] = 0\n",
    "relation_matrix[relation_matrix > 0] = 1\n",
    "relation_matrix = csr_matrix(relation_matrix)\n",
    "print(len(user_mapping), len(item_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process review content\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# generate the whole document\n",
    "all_review = []\n",
    "for item_id in item_mapping:\n",
    "    all_review.append([review_dict[item_id]])\n",
    "\n",
    "# use nltk to remove stopwords, and stemming each word\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "porter_stemmer = nltk.PorterStemmer()\n",
    "\n",
    "review_str = []\n",
    "for i, movie in enumerate(all_review):\n",
    "    # Use regular expressions to do a find-and-replace\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\",  # The pattern to search for\n",
    "                          \" \",  # The pattern to replace it with\n",
    "                          movie[0])  # The text to search\n",
    "    # print letters_only\n",
    "\n",
    "    letters_only = letters_only.lower()\n",
    "    tokens = nltk.word_tokenize(letters_only)\n",
    "\n",
    "    tokens = [w for w in tokens if w.lower() not in stopwords_set]\n",
    "    # print tokens\n",
    "\n",
    "    porter = [porter_stemmer.stem(t) for t in tokens]\n",
    "    # print porter\n",
    "    all_review[i] = porter\n",
    "    review_str.append(' '.join(porter))\n",
    "\n",
    "print review_str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to bag-of-words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, min_df=3)\n",
    "word_counts = vectorizer.fit_transform(review_str)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "print len(vocab)\n",
    "print word_counts.data.max()\n",
    "print word_counts.data.min()\n",
    "print len(item_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix.nnz / float(len(user_mapping) * len(item_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store bag-of-words to file\n",
    "def vocabulary_to_file(vocab):\n",
    "    f0 = open('vocabulary.txt', 'w')\n",
    "\n",
    "    for word in vocab:\n",
    "        f0.write(word + '\\n')\n",
    "    f0.close()\n",
    "\n",
    "\n",
    "def word_count_to_file(item_list, word_count):\n",
    "    f0 = open('word_counts.txt', 'w')\n",
    "    for i, document in enumerate(word_count):\n",
    "        indices = document.indices\n",
    "        counts = document.data\n",
    "        num_words = document.count_nonzero()\n",
    "\n",
    "        f0.write(str(item_list[i]) + ' ' + str(num_words))\n",
    "        for j, indice in enumerate(indices):\n",
    "            f0.write(' ' + str(indice) + ':' + str(counts[j]))\n",
    "        f0.write('\\n')\n",
    "    f0.close()\n",
    "\n",
    "vocabulary_to_file(vocab)\n",
    "word_count_to_file(item_mapping, word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(user_item_dict, 'Books_user_records')\n",
    "save_obj(user_mapping, 'Books_user_mapping')\n",
    "save_obj(item_mapping, 'Books_item_mapping')\n",
    "save_obj(relation_matrix, 'item_relation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print vocab[:10]\n",
    "print all_review[-1]\n",
    "print review_str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = dict()\n",
    "for w_id, word in enumerate(vocab):\n",
    "    word_to_index[word] = w_id\n",
    "\n",
    "all_review_index = []\n",
    "for i in range(len(review_str)):\n",
    "    cur_review = review_str[i].split(' ')\n",
    "    cur_index = []\n",
    "    for word in cur_review:\n",
    "        if word in word_to_index:\n",
    "            cur_index.append(word_to_index[word])\n",
    "    all_review_index.append(cur_index)\n",
    "    \n",
    "print all_review_index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store word sequence to a file\n",
    "save_obj(all_review_index, 'review_word_sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print seq_data[-1]\n",
    "user_inverse_mapping = generate_inverse_mapping(user_mapping)\n",
    "item_inverse_mapping = generate_inverse_mapping(item_mapping)\n",
    "print user_item_dict[user_mapping[-1]]\n",
    "tmp = []\n",
    "for item_id in seq_data[-1]:\n",
    "    tmp.append(item_inverse_mapping[item_id])\n",
    "print sorted(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print all_review[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(word_counts.shape[0]):\n",
    "    if word_counts.getrow(i).getnnz() == 0:\n",
    "        print i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}